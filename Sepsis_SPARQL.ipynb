{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoD2m41Rva1801bHM+dZ7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yutao-data/Sepsis_Gene_KnowledgeGraph_Ontology/blob/main/Sepsis_SPARQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS1T0IoxbsBr",
        "outputId": "70ab8280-8578-4310-fae8-ea8e4aba3b56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!wget -v https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "if os.path.exists('spark-3.5.1-bin-hadoop3.tgz'):\n",
        "    print(\"Spark tarball downloaded successfully.\")\n",
        "    !tar -xzf spark-3.5.1-bin-hadoop3.tgz -C /usr/local/\n",
        "\n",
        "    if os.path.exists('/usr/local/spark-3.5.1-bin-hadoop3'):\n",
        "        !rm -rf /usr/local/spark\n",
        "        !mv /usr/local/spark-3.5.1-bin-hadoop3 /usr/local/spark\n",
        "        print(\"Spark moved to /usr/local/spark\")\n",
        "    else:\n",
        "        print(\"Failed to extract Spark tarball.\")\n",
        "else:\n",
        "    print(\"Failed to download the Spark tarball. Please check the URL or your internet connection.\")\n",
        "\n",
        "!wget -v https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "\n",
        "if os.path.exists('hadoop-3.3.6.tar.gz'):\n",
        "    print(\"Hadoop tarball downloaded successfully.\")\n",
        "    !tar -xzf hadoop-3.3.6.tar.gz -C /usr/local/\n",
        "\n",
        "    if os.path.exists('/usr/local/hadoop-3.3.6'):\n",
        "        !rm -rf /usr/local/hadoop\n",
        "        !mv /usr/local/hadoop-3.3.6 /usr/local/hadoop\n",
        "        print(\"Hadoop moved to /usr/local/hadoop\")\n",
        "    else:\n",
        "        print(\"Failed to extract Hadoop tarball.\")\n",
        "else:\n",
        "    print(\"Failed to download the Hadoop tarball. Please check the URL or your internet connection.\")\n",
        "\n",
        "java_home_path = os.popen('readlink -f /usr/bin/java | sed \"s:bin/java::\"').read().strip()\n",
        "\n",
        "if java_home_path:\n",
        "    with open('/usr/local/spark/conf/spark-env.sh', 'a') as f:\n",
        "        f.write(f\"export JAVA_HOME={java_home_path}\\n\")\n",
        "    with open('/usr/local/hadoop/etc/hadoop/hadoop-env.sh', 'a') as f:\n",
        "        f.write(f\"export JAVA_HOME={java_home_path}\\n\")\n",
        "    print(\"JAVA_HOME added to /usr/local/spark/conf/spark-env.sh and /usr/local/hadoop/etc/hadoop/hadoop-env.sh\")\n",
        "else:\n",
        "    print(\"Failed to find JAVA_HOME.\")\n",
        "\n",
        "os.environ['JAVA_HOME'] = java_home_path\n",
        "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
        "os.environ['HADOOP_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['PATH'] = f\"/usr/local/spark/bin:/usr/local/hadoop/bin:{os.environ['PATH']}\"\n",
        "\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init('/usr/local/spark')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab Spark Test\").getOrCreate()\n",
        "print(\"Spark session created successfully\")\n",
        "spark.stop()\n",
        "\n",
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MZfokeG7C-A",
        "outputId": "7c12465d-fdcd-49d0-c7e7-6b7c43873780"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 20:17:50--  https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400446614 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.1-bin-hadoop3.tgz.12’\n",
            "\n",
            "spark-3.5.1-bin-had 100%[===================>] 381.90M  24.2MB/s    in 17s     \n",
            "\n",
            "2024-07-28 20:18:08 (21.9 MB/s) - ‘spark-3.5.1-bin-hadoop3.tgz.12’ saved [400446614/400446614]\n",
            "\n",
            "Spark tarball downloaded successfully.\n",
            "Spark moved to /usr/local/spark\n",
            "--2024-07-28 20:18:14--  https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz.3’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  22.6MB/s    in 33s     \n",
            "\n",
            "2024-07-28 20:18:47 (21.1 MB/s) - ‘hadoop-3.3.6.tar.gz.3’ saved [730107476/730107476]\n",
            "\n",
            "Hadoop tarball downloaded successfully.\n",
            "Hadoop moved to /usr/local/hadoop\n",
            "JAVA_HOME added to /usr/local/spark/conf/spark-env.sh and /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n",
            "Spark session created successfully\n",
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop version\n",
        "\n",
        "!spark-shell --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8hoZmLx8CWn",
        "outputId": "d558177a-5de1-4fe9-c2ae-1cf99531f1a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.23\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package list and install opam\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install -qq opam\n",
        "\n",
        "# Initialize opam and install packages\n",
        "!opam init --disable-sandboxing --yes\n",
        "!opam switch create 4.13.1\n",
        "!eval $(opam env)\n",
        "!opam install menhir yojson --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxGUqo8bzclb",
        "outputId": "bc66bf80-fc9e-453d-cd48-647771ef6fb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: https://repo.scala-sbt.org/scalasbt/debian/dists/all/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: https://repo.scala-sbt.org/scalasbt/debian/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "\n",
            "\u001b[36m<><>\u001b[0m \u001b[01mRequired setup - please read\u001b[0m \u001b[36m<><><><><><><><><><><><><><><><><><><><><><><>\u001b[0m\n",
            "\n",
            "  In normal operation, opam only alters files within ~/.opam.\n",
            "\n",
            "  However, to best integrate with your system, some environment variables\n",
            "  should be set. If you allow it to, this initialisation step will update\n",
            "  your \u001b[01mbash\u001b[0m configuration by adding the following line to \u001b[36m~/.profile\u001b[0m:\n",
            "\n",
            "    \u001b[01mtest -r /root/.opam/opam-init/init.sh && . /root/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\n",
            "\u001b[0m\n",
            "  Otherwise, every time you want to access your opam installation, you will\n",
            "  need to run:\n",
            "\n",
            "    \u001b[01meval $(opam env)\u001b[0m\n",
            "\n",
            "  You can always re-run this setup with 'opam init' later.\n",
            "\n",
            "\u001b[33m[WARNING]\u001b[0m Shell not updated in non-interactive mode: use --shell-setup\n",
            "A hook can be added to opam's init scripts to ensure that the shell remains in sync with the opam\n",
            "environment when they are loaded. Set that up? [y/N] y\n",
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "\u001b[31m[ERROR]\u001b[0m There already is an installed switch named 4.13.1\n",
            "[WARNING] Running as root is not recommended\n",
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package yojson is already installed (current version is 2.2.2).\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package menhir is already installed (current version is 20240715).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sbt\n",
        "!echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" | tee /etc/apt/sources.list.d/sbt.list\n",
        "!echo \"deb https://repo.scala-sbt.org/scalasbt/debian /\" | tee /etc/apt/sources.list.d/sbt_old.list\n",
        "!curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x99E82A75642AC823\" | apt-key add\n",
        "!apt-get update\n",
        "!apt-get install sbt\n",
        "\n",
        "# Install menhir and yojson via opam\n",
        "!opam install menhir yojson ocamlfind --yes\n",
        "!eval $(opam env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlE3Yosd4AfT",
        "outputId": "50edee7e-ddfb-4a6f-b83d-438499c9f0b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://repo.scala-sbt.org/scalasbt/debian all main\n",
            "deb https://repo.scala-sbt.org/scalasbt/debian /\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Ign:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 https://scala.jfrog.io/artifactory/debian all InRelease\n",
            "Ign:10 https://scala.jfrog.io/artifactory/debian  InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://scala.jfrog.io/artifactory/debian  Release\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: https://repo.scala-sbt.org/scalasbt/debian/dists/all/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: https://repo.scala-sbt.org/scalasbt/debian/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "sbt is already the newest version (1.10.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package ocamlfind is already installed (current version is 1.9.6).\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package yojson is already installed (current version is 2.2.2).\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package menhir is already installed (current version is 20240715).\n",
            "[WARNING] Running as root is not recommended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the existing sparqlgx directory\n",
        "!rm -rf sparqlgx\n",
        "\n",
        "# Clone the SPARQLGX repository\n",
        "!git clone https://github.com/tyrex-team/sparqlgx.git\n",
        "\n",
        "# Navigate to the SPARQLGX directory and update the Makefile to include --infer flag\n",
        "!sed -i 's/menhir -v Parser.mly/menhir --infer -v Parser.mly/' sparqlgx/Makefile\n",
        "\n",
        "# Verify the Makefile change\n",
        "!grep 'menhir --infer -v Parser.mly' sparqlgx/Makefile\n",
        "\n",
        "# Ensure the opam environment is loaded within the script and then run the setup scripts\n",
        "!cd sparqlgx && eval $(opam env) && bash dependencies.sh && bash compile.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cIoUbkazcn5",
        "outputId": "dead7af1-00c9-4d0d-e536-b6e8b06a1c52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sparqlgx'...\n",
            "remote: Enumerating objects: 2536, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 2536 (delta 0), reused 1 (delta 0), pack-reused 2530\u001b[K\n",
            "Receiving objects: 100% (2536/2536), 2.33 MiB | 4.57 MiB/s, done.\n",
            "Resolving deltas: 100% (1287/1287), done.\n",
            "sed: can't read sparqlgx/Makefile: No such file or directory\n",
            "grep: sparqlgx/Makefile: No such file or directory\n",
            "[WARNING] Running as root is not recommended\n",
            "Checking dependencies...\n",
            "[Success]\tsbt exists.\n",
            "[Success]\tmenhir exists.\n",
            "[Success]\tocamlc exists.\n",
            "[Success]\tocamldep exists.\n",
            "[Success]\tocamllex exists.\n",
            "[Success]\tocamlopt exists.\n",
            "[Success]\thadoop exists.\n",
            "[Success]\tspark-submit exists.\n",
            "[Success]\tocamlfind exists.\n",
            "[Success]\tyojson exists.\n",
            "\n",
            "====== SPARQLGX compilation chain ======\n",
            "---- Generation of build.sbt ----\n",
            "build.sbt in src/load-src/\n",
            "---- Load Source Compilation ----\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator (file:/root/.sbt/boot/scala-2.12.19/org.scala-sbt/sbt/1.10.1/jline-terminal-3.24.1.jar) to constructor java.lang.ProcessBuilder$RedirectPipeImpl()\n",
            "WARNING: Please consider reporting this to the maintainers of org.jline.terminal.impl.exec.ExecTerminalProvider$ReflectionRedirectPipeCreator\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mUpdated file /content/sparqlgx/src/load-src/project/build.properties: set sbt.version to 1.10.1\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mwelcome to sbt 1.10.1 (Ubuntu Java 11.0.23)\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mloading project definition from /content/sparqlgx/src/load-src/project\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / csrConfiguration 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / update 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / update 1s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / update 2s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src-build / Compile / compileIncremental 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mloading settings for project load-src from build.sbt ...\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mset current project to SPARQLGX Load (in build file:/content/sparqlgx/src/load-src/)\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / dependencyPositions 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 1s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 2s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 3s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 3s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 4s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 5s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / update 6s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcompiling 1 Scala source to /content/sparqlgx/src/load-src/target/scala-2.11/classes ...\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 0s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 1s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 2s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 3s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 4s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 5s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 6s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 7s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 7s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 8s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 9s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 10s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 11s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 11s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 12s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 13s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 14s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 15s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[1000D\n",
            "\u001b[2K\u001b[2K  | => load-src / Compile / compileIncremental 16s\n",
            "\u001b[2K\u001b[1A\u001b[1000D\u001b[0J\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 26 s, completed Jul 28, 2024, 8:20:59 PM\u001b[0m\u001b[0J\n",
            "\u001b[0J\u001b[0J---- Translation Source Compilation ----\n",
            "Makefile:36: warning: ignoring prerequisites on suffix rule definition\n",
            "Makefile:42: warning: ignoring prerequisites on suffix rule definition\n",
            "Makefile:39: warning: ignoring prerequisites on suffix rule definition\n",
            "ocamllex Lexer.mll\n",
            "23 states, 1029 transitions, table size 4254 bytes\n",
            "menhir -v Parser.mly\n",
            "Warning: 5 states have shift/reduce conflicts.\n",
            "Warning: 3 states have reduce/reduce conflicts.\n",
            "Warning: 17 shift/reduce conflicts were arbitrarily resolved.\n",
            "Warning: 4 reduce/reduce conflicts were arbitrarily resolved.\n",
            "Error: the code back-end requires the type of every nonterminal symbol to be\n",
            "known. Please specify the type of every symbol via %type declarations, or\n",
            "enable type inference (look up --infer in the manual).\n",
            "Type inference is automatically enabled when Menhir is used via Dune,\n",
            "provided the dune-project file says (using menhir 2.0) or later.\n",
            "The types of the following nonterminal symbols are unknown:\n",
            "complex_toplevel\n",
            "distinct\n",
            "filter\n",
            "filter_expr\n",
            "ident\n",
            "ident_or_var\n",
            "list(VAR)\n",
            "list(prefix)\n",
            "loption(separated_nonempty_list(COLON,IDENT))\n",
            "loption(separated_nonempty_list(COMMA,VAR))\n",
            "nonempty_list(orderlist)\n",
            "option(distinct)\n",
            "option(orderby)\n",
            "orderby\n",
            "orderlist\n",
            "prefix\n",
            "ptp\n",
            "separated_nonempty_list(COLON,IDENT)\n",
            "separated_nonempty_list(COMMA,VAR)\n",
            "toplevel\n",
            "tp\n",
            "vars\n",
            "Makefile:52: .depend: No such file or directory\n",
            "make: *** [Makefile:39: Parser.ml] Error 1\n",
            "chmod: cannot access 'sparql2scala': No such file or directory\n",
            "cp: cannot stat 'sparql2scala': No such file or directory\n",
            "========================================\n",
            "\n",
            "The compilation process took 58 seconds.\n",
            "\n",
            "Before using SPARQLGX, make sure that:\n",
            "  1. The conf file in conf/ is correct\n",
            "  2. alias bin/sparqlgx.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!opam install menhir --yes\n",
        "!eval $(opam env)\n",
        "!which menhir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yph6lDNT64Qh",
        "outputId": "44476c48-c421-4d28-ebea-ffeb400774f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "\u001b[1;34m[NOTE]\u001b[0m Package menhir is already installed (current version is 20240715).\n",
            "[WARNING] Running as root is not recommended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop fs -mkdir -p /user/root/input\n",
        "!hadoop fs -put /content/drive/MyDrive/stagelisn2024-main/Owl/enriched_ppio_ontology_with_kegg_Pathways.owl /user/root/input/"
      ],
      "metadata": {
        "id": "W3qwiuMyzcqp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sparqlgx && bash bin/sparqlgx.sh load myDatabase /user/root/input/enriched_ppio_ontology_with_kegg_Pathways.owl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8Q8gZ1Xzcs4",
        "outputId": "96677b45-fea3-4569-a4fa-238a30c63495"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/07/28 20:21:20 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/07/28 20:21:20 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:21:20 INFO SparkContext: Java version 11.0.23\n",
            "24/07/28 20:21:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/07/28 20:21:21 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:21:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/07/28 20:21:21 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:21:21 INFO SparkContext: Submitted application: Simple Application\n",
            "24/07/28 20:21:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/07/28 20:21:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/07/28 20:21:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/07/28 20:21:21 INFO SecurityManager: Changing view acls to: root\n",
            "24/07/28 20:21:21 INFO SecurityManager: Changing modify acls to: root\n",
            "24/07/28 20:21:21 INFO SecurityManager: Changing view acls groups to: \n",
            "24/07/28 20:21:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/07/28 20:21:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/07/28 20:21:22 INFO Utils: Successfully started service 'sparkDriver' on port 33695.\n",
            "24/07/28 20:21:22 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/07/28 20:21:22 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/07/28 20:21:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/07/28 20:21:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/07/28 20:21:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/07/28 20:21:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-16641d43-e543-4b06-b38c-fac9a3752d58\n",
            "24/07/28 20:21:22 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n",
            "24/07/28 20:21:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/07/28 20:21:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/07/28 20:21:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/07/28 20:21:23 INFO SparkContext: Added JAR file:/content/sparqlgx/bin/sparqlgx-load.jar at spark://4ae5c1b8e490:33695/jars/sparqlgx-load.jar with timestamp 1722198080229\n",
            "24/07/28 20:21:23 INFO Executor: Starting executor ID driver on host 4ae5c1b8e490\n",
            "24/07/28 20:21:23 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:21:23 INFO Executor: Java version 11.0.23\n",
            "24/07/28 20:21:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/07/28 20:21:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e3ecf5c for default.\n",
            "24/07/28 20:21:23 INFO Executor: Fetching spark://4ae5c1b8e490:33695/jars/sparqlgx-load.jar with timestamp 1722198080229\n",
            "24/07/28 20:21:23 INFO TransportClientFactory: Successfully created connection to 4ae5c1b8e490/172.28.0.12:33695 after 75 ms (0 ms spent in bootstraps)\n",
            "24/07/28 20:21:23 INFO Utils: Fetching spark://4ae5c1b8e490:33695/jars/sparqlgx-load.jar to /tmp/spark-bac763c8-d19e-453e-a12c-415ce2f10a6f/userFiles-2786541d-6141-431a-b06d-e1791d62ecaa/fetchFileTemp8931804467553493923.tmp\n",
            "24/07/28 20:21:23 INFO Executor: Adding file:/tmp/spark-bac763c8-d19e-453e-a12c-415ce2f10a6f/userFiles-2786541d-6141-431a-b06d-e1791d62ecaa/sparqlgx-load.jar to class loader default\n",
            "24/07/28 20:21:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39857.\n",
            "24/07/28 20:21:23 INFO NettyBlockTransferService: Server created on 4ae5c1b8e490:39857\n",
            "24/07/28 20:21:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/07/28 20:21:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ae5c1b8e490, 39857, None)\n",
            "24/07/28 20:21:23 INFO BlockManagerMasterEndpoint: Registering block manager 4ae5c1b8e490:39857 with 1048.8 MiB RAM, BlockManagerId(driver, 4ae5c1b8e490, 39857, None)\n",
            "24/07/28 20:21:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ae5c1b8e490, 39857, None)\n",
            "24/07/28 20:21:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ae5c1b8e490, 39857, None)\n",
            "24/07/28 20:21:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:21:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:21:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4ae5c1b8e490:39857 (size: 32.6 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:21:25 INFO SparkContext: Created broadcast 0 from textFile at Load.scala:351\n",
            "24/07/28 20:21:26 INFO FileInputFormat: Total input files to process : 1\n",
            "24/07/28 20:21:26 INFO SparkContext: Starting job: reduce at Load.scala:223\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Registering RDD 5 (map at Load.scala:221) as input to shuffle 0\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Got job 0 (reduce at Load.scala:223) with 5 output partitions\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at Load.scala:223)\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221), which has no missing parents\n",
            "24/07/28 20:21:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.4 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:21:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:21:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4ae5c1b8e490:39857 (size: 4.2 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:21:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/07/28 20:21:27 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
            "24/07/28 20:21:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 5 tasks resource profile 0\n",
            "24/07/28 20:21:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4ae5c1b8e490, executor driver, partition 0, PROCESS_LOCAL, 7863 bytes) \n",
            "24/07/28 20:21:27 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490, executor driver, partition 1, PROCESS_LOCAL, 7863 bytes) \n",
            "24/07/28 20:21:27 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/07/28 20:21:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/07/28 20:21:27 INFO HadoopRDD: Input split: file:/user/root/input/enriched_ppio_ontology_with_kegg_Pathways.owl:33554432+33554432\n",
            "24/07/28 20:21:27 INFO HadoopRDD: Input split: file:/user/root/input/enriched_ppio_ontology_with_kegg_Pathways.owl:0+33554432\n",
            "24/07/28 20:21:27 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
            "java.lang.RuntimeException: Invalid line: <?xml version=\"1.0\"?>\n",
            "\tat Main$$anonfun$29.apply(Load.scala:355)\n",
            "\tat Main$$anonfun$29.apply(Load.scala:352)\n",
            "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
            "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/07/28 20:21:27 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\n",
            "java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/07/28 20:21:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (4ae5c1b8e490, executor driver, partition 2, PROCESS_LOCAL, 7863 bytes) \n",
            "24/07/28 20:21:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "24/07/28 20:21:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (4ae5c1b8e490, executor driver, partition 3, PROCESS_LOCAL, 7863 bytes) \n",
            "24/07/28 20:21:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "24/07/28 20:21:28 INFO HadoopRDD: Input split: file:/user/root/input/enriched_ppio_ontology_with_kegg_Pathways.owl:100663296+33554432\n",
            "24/07/28 20:21:28 INFO HadoopRDD: Input split: file:/user/root/input/enriched_ppio_ontology_with_kegg_Pathways.owl:67108864+33554432\n",
            "24/07/28 20:21:28 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490 executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "24/07/28 20:21:28 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job\n",
            "24/07/28 20:21:28 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (4ae5c1b8e490 executor driver): java.lang.RuntimeException: Invalid line: <?xml version=\"1.0\"?>\n",
            "\tat Main$$anonfun$29.apply(Load.scala:355)\n",
            "\tat Main$$anonfun$29.apply(Load.scala:352)\n",
            "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
            "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "24/07/28 20:21:28 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)\n",
            "java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/07/28 20:21:28 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
            "java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/07/28 20:21:28 INFO TaskSchedulerImpl: Cancelling stage 0\n",
            "24/07/28 20:21:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490 executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "24/07/28 20:21:28 INFO TaskSchedulerImpl: Stage 0 was cancelled\n",
            "24/07/28 20:21:28 INFO DAGScheduler: ShuffleMapStage 0 (map at Load.scala:221) failed in 0.899 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490 executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "24/07/28 20:21:28 INFO TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) on 4ae5c1b8e490, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 0) [duplicate 1]\n",
            "24/07/28 20:21:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:21:28 INFO TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) on 4ae5c1b8e490, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 0) [duplicate 2]\n",
            "24/07/28 20:21:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:21:28 INFO DAGScheduler: Job 0 failed: reduce at Load.scala:223, took 1.930354 s\n",
            "Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490 executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1139)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1121)\n",
            "\tat Main$.prefix(Load.scala:223)\n",
            "\tat Main$.main(Load.scala:365)\n",
            "\tat Main.main(Load.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 0\n",
            "\tat java.base/java.lang.StringLatin1.charAt(StringLatin1.java:47)\n",
            "\tat java.base/java.lang.String.charAt(String.java:693)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat Main$$anonfun$18.apply(Load.scala:220)\n",
            "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
            "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/07/28 20:21:28 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/07/28 20:21:28 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/07/28 20:21:28 INFO SparkUI: Stopped Spark web UI at http://4ae5c1b8e490:4040\n",
            "24/07/28 20:21:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/07/28 20:21:28 INFO MemoryStore: MemoryStore cleared\n",
            "24/07/28 20:21:28 INFO BlockManager: BlockManager stopped\n",
            "24/07/28 20:21:28 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/07/28 20:21:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/07/28 20:21:28 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/07/28 20:21:28 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/07/28 20:21:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-bac763c8-d19e-453e-a12c-415ce2f10a6f\n",
            "24/07/28 20:21:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-35579228-a6c1-43f1-bae2-30ec7766c4a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ocaml -version\n",
        "!opam list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNFLhwjY3rlb",
        "outputId": "c9f7095a-41ca-4768-a99f-5ae4ac18a875"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The OCaml toplevel, version 4.13.1\n",
            "\u001b[33m[WARNING]\u001b[0m Running as root is not recommended\n",
            "# Packages matching: \u001b[36minstalled\u001b[0m\n",
            "# Name            # Installed # Synopsis\n",
            "\u001b[01mbase-bigarray\u001b[0m     \u001b[35mbase\u001b[0m\n",
            "\u001b[01mbase-threads\u001b[0m      \u001b[35mbase\u001b[0m\n",
            "\u001b[01mbase-unix\u001b[0m         \u001b[35mbase\u001b[0m\n",
            "\u001b[01mdune\u001b[0m              \u001b[35m3.16.0\u001b[0m      Fast, portable, and opinionated build system\n",
            "\u001b[01mhost-arch-x86_64\u001b[0m  \u001b[35m1\u001b[0m           OCaml on amd64 (64-bit)\n",
            "\u001b[01mhost-system-other\u001b[0m \u001b[35m1\u001b[0m           OCaml on an unidentified system\n",
            "\u001b[01;04mmenhir\u001b[0m            \u001b[35m20240715\u001b[0m    An LR(1) parser generator\n",
            "\u001b[01mmenhirCST\u001b[0m         \u001b[35m20240715\u001b[0m    Runtime support library for parsers generated by Menhir\n",
            "\u001b[01mmenhirLib\u001b[0m         \u001b[35m20240715\u001b[0m    Runtime support library for parsers generated by Menhir\n",
            "\u001b[01mmenhirSdk\u001b[0m         \u001b[35m20240715\u001b[0m    Compile-time library for auxiliary tools related to Menhir\n",
            "\u001b[01mocaml\u001b[0m             \u001b[35m4.13.1\u001b[0m      The OCaml compiler (virtual package)\n",
            "\u001b[01mocaml-config\u001b[0m      \u001b[35m2\u001b[0m           OCaml Switch Configuration\n",
            "\u001b[01;04mocaml-system\u001b[0m      \u001b[35m4.13.1\u001b[0m      The OCaml compiler (system version, from outside of opam)\n",
            "\u001b[01;04mocamlfind\u001b[0m         \u001b[35m1.9.6\u001b[0m       A library manager for OCaml\n",
            "\u001b[01mseq\u001b[0m               \u001b[35mbase\u001b[0m        Compatibility package for OCaml's standard iterator type starting from\n",
            "\u001b[01;04myojson\u001b[0m            \u001b[35m2.2.2\u001b[0m       Yojson is an optimized parsing and printing library for the JSON forma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_command(command, expected_output=None, is_error_expected=False):\n",
        "    try:\n",
        "        result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        output = result.stderr.decode().strip() if is_error_expected else result.stdout.decode().strip()\n",
        "        if expected_output and expected_output not in output:\n",
        "            print(f\"Command '{command}' executed but did not find expected output: '{expected_output}'\")\n",
        "            print(f\"Output: {output}\")\n",
        "            return False\n",
        "        print(f\"Command '{command}' executed successfully.\")\n",
        "        print(f\"Output: {output}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing command '{command}': {e.stderr.decode().strip()}\")\n",
        "        return False\n",
        "\n",
        "def check_java():\n",
        "    return check_command(\"java -version\", \"openjdk\", is_error_expected=True)\n",
        "\n",
        "def check_sbt():\n",
        "    return check_command(\"sbt --version\", \"sbt version\")\n",
        "\n",
        "def check_spark():\n",
        "    spark_home = os.getenv('SPARK_HOME')\n",
        "    if not spark_home:\n",
        "        print(\"SPARK_HOME environment variable is not set.\")\n",
        "        return False\n",
        "    return check_command(f\"{spark_home}/bin/spark-submit --version\", \"version\", is_error_expected=True)\n",
        "\n",
        "def check_sparqlgx():\n",
        "    if not os.path.exists(\"sparqlgx\"):\n",
        "        print(\"sparqlgx directory does not exist.\")\n",
        "        return False\n",
        "    return check_command(\"cd sparqlgx && sbt clean package\", \"success\")\n",
        "\n",
        "def main():\n",
        "    print(\"Checking Java installation...\")\n",
        "    java_installed = check_java()\n",
        "\n",
        "    print(\"Checking SBT installation...\")\n",
        "    sbt_installed = check_sbt()\n",
        "\n",
        "    print(\"Checking Spark installation...\")\n",
        "    spark_installed = check_spark()\n",
        "\n",
        "    print(\"Checking SPARQLGX installation...\")\n",
        "    sparqlgx_installed = check_sparqlgx()\n",
        "\n",
        "    if java_installed and sbt_installed and spark_installed and sparqlgx_installed:\n",
        "        print(\"All required environments are correctly installed.\")\n",
        "    else:\n",
        "        print(\"Some environments are not correctly installed. Please check the above logs for details.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59d37nbBIVU",
        "outputId": "f3485955-753c-48a0-a957-e871c8910a36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Java installation...\n",
            "Command 'java -version' executed successfully.\n",
            "Output: openjdk version \"11.0.23\" 2024-04-16\n",
            "OpenJDK Runtime Environment (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Checking SBT installation...\n",
            "Command 'sbt --version' executed successfully.\n",
            "Output: sbt version in this project: 1.10.1\n",
            "sbt script version: 1.10.1\n",
            "Checking Spark installation...\n",
            "Command '/usr/local/spark/bin/spark-submit --version' executed successfully.\n",
            "Output: Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.23\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "Checking SPARQLGX installation...\n",
            "Command 'cd sparqlgx && sbt clean package' executed successfully.\n",
            "Output: [warn] No sbt.version set in project/build.properties, base directory: /content/sparqlgx\n",
            "[info] welcome to sbt 1.10.1 (Ubuntu Java 11.0.23)\n",
            "[info] set current project to sparqlgx (in build file:/content/sparqlgx/)\n",
            "[success] Total time: 1 s, completed Jul 28, 2024, 8:34:54 PM\n",
            "[success] Total time: 3 s, completed Jul 28, 2024, 8:34:57 PM\n",
            "All required environments are correctly installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HDFS_NAMENODE_USER'] = 'root'\n",
        "os.environ['HDFS_DATANODE_USER'] = 'root'\n",
        "os.environ['HDFS_SECONDARYNAMENODE_USER'] = 'root'\n",
        "\n",
        "!echo 'Y' | hdfs namenode -format\n",
        "\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "\n",
        "!jps\n",
        "\n",
        "!hadoop fs -rm -r /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
        "\n",
        "!hadoop fs -mkdir -p /rdf_data\n",
        "!hadoop fs -copyFromLocal /content/enriched_ppio_ontology_with_kegg_Pathways.nt /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
        "\n",
        "!cd sparqlgx && bash bin/sparqlgx.sh light-load myDatabase /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
        "!cd sparqlgx && bash bin/sparqlgx.sh generate-stat myDatabase /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
        "!cd sparqlgx && bash bin/sparqlgx.sh load myDatabase /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
        "\n",
        "sparql_query = \"\"\"\n",
        "SELECT ?s ?p ?o\n",
        "WHERE {\n",
        "  ?s ?p ?o .\n",
        "}\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sparqlgx/query.rq\", \"w\") as file:\n",
        "    file.write(sparql_query)\n",
        "\n",
        "!cd sparqlgx && bash bin/sparqlgx.sh query myDatabase sparqlgx/query.rq\n",
        "\n",
        "with open(\"sparqlgx/query_output.txt\", \"r\") as file:\n",
        "    print(file.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zD_KfPP6FgxD",
        "outputId": "397fe3ac-49ec-4e35-cf34-a818f019adfc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-28 20:47:29,507 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 4ae5c1b8e490/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
            "STARTUP_MSG:   java = 11.0.23\n",
            "************************************************************/\n",
            "2024-07-28 20:47:29,617 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-07-28 20:47:29,827 INFO namenode.NameNode: createNameNode [-format]\n",
            "2024-07-28 20:47:30,701 INFO namenode.NameNode: Formatting using clusterid: CID-ced4091c-97e0-4046-a73f-3766e672fe62\n",
            "2024-07-28 20:47:30,811 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-07-28 20:47:30,899 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-07-28 20:47:30,902 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-07-28 20:47:30,902 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-07-28 20:47:30,953 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2024-07-28 20:47:30,953 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2024-07-28 20:47:30,954 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2024-07-28 20:47:30,954 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2024-07-28 20:47:30,954 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-07-28 20:47:31,029 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-07-28 20:47:31,239 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2024-07-28 20:47:31,239 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-07-28 20:47:31,244 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-07-28 20:47:31,244 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Jul 28 20:47:31\n",
            "2024-07-28 20:47:31,247 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-07-28 20:47:31,247 INFO util.GSet: VM type       = 64-bit\n",
            "2024-07-28 20:47:31,248 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2024-07-28 20:47:31,248 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-07-28 20:47:31,281 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-07-28 20:47:31,281 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-07-28 20:47:31,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2024-07-28 20:47:31,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-07-28 20:47:31,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-07-28 20:47:31,291 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-07-28 20:47:31,291 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-07-28 20:47:31,291 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-07-28 20:47:31,291 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-07-28 20:47:31,291 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-07-28 20:47:31,292 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-07-28 20:47:31,292 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-07-28 20:47:31,340 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-07-28 20:47:31,340 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-07-28 20:47:31,340 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-07-28 20:47:31,340 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-07-28 20:47:31,357 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-07-28 20:47:31,357 INFO util.GSet: VM type       = 64-bit\n",
            "2024-07-28 20:47:31,357 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
            "2024-07-28 20:47:31,357 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-07-28 20:47:31,369 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2024-07-28 20:47:31,369 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-07-28 20:47:31,369 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-07-28 20:47:31,369 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-07-28 20:47:31,378 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-07-28 20:47:31,381 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-07-28 20:47:31,387 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-07-28 20:47:31,388 INFO util.GSet: VM type       = 64-bit\n",
            "2024-07-28 20:47:31,388 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2024-07-28 20:47:31,388 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-07-28 20:47:31,404 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-07-28 20:47:31,405 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-07-28 20:47:31,405 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-07-28 20:47:31,417 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-07-28 20:47:31,418 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-07-28 20:47:31,420 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-07-28 20:47:31,420 INFO util.GSet: VM type       = 64-bit\n",
            "2024-07-28 20:47:31,421 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n",
            "2024-07-28 20:47:31,421 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "Re-format filesystem in Storage Directory root= /tmp/hadoop-root/dfs/name; location= null ? (Y or N) 2024-07-28 20:47:31,463 INFO namenode.FSImage: Allocated new BlockPoolId: BP-206968037-172.28.0.12-1722199651451\n",
            "2024-07-28 20:47:31,463 INFO common.Storage: Will remove files: [/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000.md5, /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, /tmp/hadoop-root/dfs/name/current/VERSION, /tmp/hadoop-root/dfs/name/current/seen_txid]\n",
            "2024-07-28 20:47:31,496 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-07-28 20:47:31,560 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-07-28 20:47:31,738 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2024-07-28 20:47:31,761 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-07-28 20:47:31,818 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-07-28 20:47:31,818 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-07-28 20:47:31,847 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-07-28 20:47:31,848 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 4ae5c1b8e490/172.28.0.12\n",
            "************************************************************/\n",
            "Starting namenodes on [4ae5c1b8e490]\n",
            "4ae5c1b8e490: ssh: connect to host 4ae5c1b8e490 port 22: Connection refused\n",
            "Starting datanodes\n",
            "localhost: ssh: connect to host localhost port 22: Cannot assign requested address\n",
            "Starting secondary namenodes [4ae5c1b8e490]\n",
            "4ae5c1b8e490: ssh: connect to host 4ae5c1b8e490 port 22: Connection refused\n",
            "22289 SparkSubmit\n",
            "37844 Jps\n",
            "2024-07-28 20:47:44,359 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt\n",
            "24/07/28 20:47:54 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/07/28 20:47:54 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:47:54 INFO SparkContext: Java version 11.0.23\n",
            "24/07/28 20:47:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/07/28 20:47:54 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:47:54 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/07/28 20:47:54 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:47:54 INFO SparkContext: Submitted application: Simple Application\n",
            "24/07/28 20:47:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/07/28 20:47:54 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/07/28 20:47:54 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/07/28 20:47:55 INFO SecurityManager: Changing view acls to: root\n",
            "24/07/28 20:47:55 INFO SecurityManager: Changing modify acls to: root\n",
            "24/07/28 20:47:55 INFO SecurityManager: Changing view acls groups to: \n",
            "24/07/28 20:47:55 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/07/28 20:47:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/07/28 20:47:55 INFO Utils: Successfully started service 'sparkDriver' on port 39619.\n",
            "24/07/28 20:47:55 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/07/28 20:47:55 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/07/28 20:47:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/07/28 20:47:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/07/28 20:47:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/07/28 20:47:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cbbe6587-6ac7-4db7-90fe-900e847cc811\n",
            "24/07/28 20:47:55 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n",
            "24/07/28 20:47:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/07/28 20:47:56 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/07/28 20:47:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/07/28 20:47:56 INFO SparkContext: Added JAR file:/content/sparqlgx/bin/sparqlgx-load.jar at spark://4ae5c1b8e490:39619/jars/sparqlgx-load.jar with timestamp 1722199674464\n",
            "24/07/28 20:47:57 INFO Executor: Starting executor ID driver on host 4ae5c1b8e490\n",
            "24/07/28 20:47:57 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:47:57 INFO Executor: Java version 11.0.23\n",
            "24/07/28 20:47:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/07/28 20:47:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e3ecf5c for default.\n",
            "24/07/28 20:47:57 INFO Executor: Fetching spark://4ae5c1b8e490:39619/jars/sparqlgx-load.jar with timestamp 1722199674464\n",
            "24/07/28 20:47:58 INFO TransportClientFactory: Successfully created connection to 4ae5c1b8e490/172.28.0.12:39619 after 231 ms (0 ms spent in bootstraps)\n",
            "24/07/28 20:47:58 INFO Utils: Fetching spark://4ae5c1b8e490:39619/jars/sparqlgx-load.jar to /tmp/spark-b9f5f3ae-2dec-4386-aa73-2f5c37d2fafd/userFiles-0ef1cbbc-3e14-4927-9dbb-fed36abe4eed/fetchFileTemp11553622441570670054.tmp\n",
            "24/07/28 20:47:58 INFO Executor: Adding file:/tmp/spark-b9f5f3ae-2dec-4386-aa73-2f5c37d2fafd/userFiles-0ef1cbbc-3e14-4927-9dbb-fed36abe4eed/sparqlgx-load.jar to class loader default\n",
            "24/07/28 20:47:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43669.\n",
            "24/07/28 20:47:58 INFO NettyBlockTransferService: Server created on 4ae5c1b8e490:43669\n",
            "24/07/28 20:47:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/07/28 20:47:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ae5c1b8e490, 43669, None)\n",
            "24/07/28 20:47:58 INFO BlockManagerMasterEndpoint: Registering block manager 4ae5c1b8e490:43669 with 1048.8 MiB RAM, BlockManagerId(driver, 4ae5c1b8e490, 43669, None)\n",
            "24/07/28 20:47:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ae5c1b8e490, 43669, None)\n",
            "24/07/28 20:47:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ae5c1b8e490, 43669, None)\n",
            "24/07/28 20:48:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:48:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:48:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4ae5c1b8e490:43669 (size: 32.6 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:02 INFO SparkContext: Created broadcast 0 from textFile at Load.scala:351\n",
            "24/07/28 20:48:03 INFO FileInputFormat: Total input files to process : 1\n",
            "24/07/28 20:48:03 INFO SparkContext: Starting job: reduce at Load.scala:223\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Registering RDD 5 (map at Load.scala:221) as input to shuffle 0\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Got job 0 (reduce at Load.scala:223) with 7 output partitions\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at Load.scala:223)\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/07/28 20:48:04 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221), which has no missing parents\n",
            "24/07/28 20:48:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.4 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4ae5c1b8e490:43669 (size: 4.2 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/07/28 20:48:05 INFO DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\n",
            "24/07/28 20:48:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 7 tasks resource profile 0\n",
            "24/07/28 20:48:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4ae5c1b8e490, executor driver, partition 0, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490, executor driver, partition 1, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:05 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/07/28 20:48:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/07/28 20:48:05 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:0+33554432\n",
            "24/07/28 20:48:05 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:33554432+33554432\n",
            "24/07/28 20:48:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (4ae5c1b8e490, executor driver, partition 2, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:10 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "24/07/28 20:48:10 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:67108864+33554432\n",
            "24/07/28 20:48:10 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5479 ms on 4ae5c1b8e490 (executor driver) (1/7)\n",
            "24/07/28 20:48:10 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (4ae5c1b8e490, executor driver, partition 3, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:10 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "24/07/28 20:48:10 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5448 ms on 4ae5c1b8e490 (executor driver) (2/7)\n",
            "24/07/28 20:48:10 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:100663296+33554432\n",
            "24/07/28 20:48:13 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:13 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (4ae5c1b8e490, executor driver, partition 4, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:13 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
            "24/07/28 20:48:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2633 ms on 4ae5c1b8e490 (executor driver) (3/7)\n",
            "24/07/28 20:48:13 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:134217728+33554432\n",
            "24/07/28 20:48:13 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:13 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (4ae5c1b8e490, executor driver, partition 5, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:13 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
            "24/07/28 20:48:13 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:167772160+33554432\n",
            "24/07/28 20:48:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2990 ms on 4ae5c1b8e490 (executor driver) (4/7)\n",
            "24/07/28 20:48:16 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:16 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (4ae5c1b8e490, executor driver, partition 6, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:16 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 3381 ms on 4ae5c1b8e490 (executor driver) (5/7)\n",
            "24/07/28 20:48:16 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
            "24/07/28 20:48:16 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:201326592+28756888\n",
            "24/07/28 20:48:16 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:16 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 3306 ms on 4ae5c1b8e490 (executor driver) (6/7)\n",
            "24/07/28 20:48:18 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:18 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2145 ms on 4ae5c1b8e490 (executor driver) (7/7)\n",
            "24/07/28 20:48:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:48:18 INFO DAGScheduler: ShuffleMapStage 0 (map at Load.scala:221) finished in 13.978 s\n",
            "24/07/28 20:48:18 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/07/28 20:48:18 INFO DAGScheduler: running: Set()\n",
            "24/07/28 20:48:18 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/07/28 20:48:18 INFO DAGScheduler: failed: Set()\n",
            "24/07/28 20:48:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at map at Load.scala:223), which has no missing parents\n",
            "24/07/28 20:48:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.8 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4ae5c1b8e490:43669 (size: 3.3 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/07/28 20:48:18 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at map at Load.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\n",
            "24/07/28 20:48:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0\n",
            "24/07/28 20:48:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 7) (4ae5c1b8e490, executor driver, partition 0, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:19 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 8) (4ae5c1b8e490, executor driver, partition 1, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 7)\n",
            "24/07/28 20:48:19 INFO Executor: Running task 1.0 in stage 1.0 (TID 8)\n",
            "24/07/28 20:48:19 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:19 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 64 ms\n",
            "24/07/28 20:48:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 74 ms\n",
            "24/07/28 20:48:20 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 911.7 KiB, free 1047.6 MiB)\n",
            "24/07/28 20:48:20 INFO BlockManagerInfo: Added rdd_6_1 in memory on 4ae5c1b8e490:43669 (size: 911.7 KiB, free: 1047.9 MiB)\n",
            "24/07/28 20:48:20 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 917.8 KiB, free 1046.7 MiB)\n",
            "24/07/28 20:48:20 INFO BlockManagerInfo: Added rdd_6_0 in memory on 4ae5c1b8e490:43669 (size: 917.8 KiB, free: 1047.0 MiB)\n",
            "24/07/28 20:48:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 7). 1830 bytes result sent to driver\n",
            "24/07/28 20:48:20 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 9) (4ae5c1b8e490, executor driver, partition 2, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 7) in 1330 ms on 4ae5c1b8e490 (executor driver) (1/7)\n",
            "24/07/28 20:48:20 INFO Executor: Finished task 1.0 in stage 1.0 (TID 8). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:20 INFO Executor: Running task 2.0 in stage 1.0 (TID 9)\n",
            "24/07/28 20:48:20 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 10) (4ae5c1b8e490, executor driver, partition 3, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 8) in 1332 ms on 4ae5c1b8e490 (executor driver) (2/7)\n",
            "24/07/28 20:48:20 INFO Executor: Running task 3.0 in stage 1.0 (TID 10)\n",
            "24/07/28 20:48:20 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:20 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "24/07/28 20:48:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "24/07/28 20:48:20 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 920.8 KiB, free 1045.8 MiB)\n",
            "24/07/28 20:48:20 INFO BlockManagerInfo: Added rdd_6_3 in memory on 4ae5c1b8e490:43669 (size: 920.8 KiB, free: 1046.1 MiB)\n",
            "24/07/28 20:48:20 INFO Executor: Finished task 3.0 in stage 1.0 (TID 10). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:20 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 11) (4ae5c1b8e490, executor driver, partition 4, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:20 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 10) in 640 ms on 4ae5c1b8e490 (executor driver) (3/7)\n",
            "24/07/28 20:48:20 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 927.5 KiB, free 1044.9 MiB)\n",
            "24/07/28 20:48:20 INFO Executor: Running task 4.0 in stage 1.0 (TID 11)\n",
            "24/07/28 20:48:20 INFO BlockManagerInfo: Added rdd_6_2 in memory on 4ae5c1b8e490:43669 (size: 927.5 KiB, free: 1045.2 MiB)\n",
            "24/07/28 20:48:21 INFO Executor: Finished task 2.0 in stage 1.0 (TID 9). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:21 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 12) (4ae5c1b8e490, executor driver, partition 5, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:21 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 9) in 701 ms on 4ae5c1b8e490 (executor driver) (4/7)\n",
            "24/07/28 20:48:21 INFO Executor: Running task 5.0 in stage 1.0 (TID 12)\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Getting 7 (124.8 KiB) non-empty blocks including 7 (124.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "24/07/28 20:48:21 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 917.6 KiB, free 1044.0 MiB)\n",
            "24/07/28 20:48:21 INFO BlockManagerInfo: Added rdd_6_4 in memory on 4ae5c1b8e490:43669 (size: 917.6 KiB, free: 1044.3 MiB)\n",
            "24/07/28 20:48:21 INFO Executor: Finished task 4.0 in stage 1.0 (TID 11). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:21 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 921.4 KiB, free 1043.1 MiB)\n",
            "24/07/28 20:48:21 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 13) (4ae5c1b8e490, executor driver, partition 6, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:21 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 11) in 558 ms on 4ae5c1b8e490 (executor driver) (5/7)\n",
            "24/07/28 20:48:21 INFO BlockManagerInfo: Added rdd_6_5 in memory on 4ae5c1b8e490:43669 (size: 921.4 KiB, free: 1043.4 MiB)\n",
            "24/07/28 20:48:21 INFO Executor: Running task 6.0 in stage 1.0 (TID 13)\n",
            "24/07/28 20:48:21 INFO Executor: Finished task 5.0 in stage 1.0 (TID 12). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Getting 7 (124.8 KiB) non-empty blocks including 7 (124.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "24/07/28 20:48:21 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 12) in 554 ms on 4ae5c1b8e490 (executor driver) (6/7)\n",
            "24/07/28 20:48:21 INFO MemoryStore: Block rdd_6_6 stored as values in memory (estimated size 913.5 KiB, free 1042.3 MiB)\n",
            "24/07/28 20:48:21 INFO BlockManagerInfo: Added rdd_6_6 in memory on 4ae5c1b8e490:43669 (size: 913.5 KiB, free: 1042.5 MiB)\n",
            "24/07/28 20:48:21 INFO Executor: Finished task 6.0 in stage 1.0 (TID 13). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:21 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 13) in 280 ms on 4ae5c1b8e490 (executor driver) (7/7)\n",
            "24/07/28 20:48:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:48:21 INFO DAGScheduler: ResultStage 1 (reduce at Load.scala:223) finished in 2.879 s\n",
            "24/07/28 20:48:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/07/28 20:48:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/07/28 20:48:21 INFO DAGScheduler: Job 0 finished: reduce at Load.scala:223, took 18.282723 s\n",
            "Exception in thread \"main\" java.lang.NoSuchMethodError: 'scala.collection.mutable.ArrayOps scala.Predef$.refArrayOps(java.lang.Object[])'\n",
            "\tat Main$.prefix(Load.scala:231)\n",
            "\tat Main$.main(Load.scala:365)\n",
            "\tat Main.main(Load.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "24/07/28 20:48:21 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/07/28 20:48:21 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/07/28 20:48:21 INFO SparkUI: Stopped Spark web UI at http://4ae5c1b8e490:4040\n",
            "24/07/28 20:48:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/07/28 20:48:21 INFO MemoryStore: MemoryStore cleared\n",
            "24/07/28 20:48:21 INFO BlockManager: BlockManager stopped\n",
            "24/07/28 20:48:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/07/28 20:48:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/07/28 20:48:21 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/07/28 20:48:21 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/07/28 20:48:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-b9f5f3ae-2dec-4386-aa73-2f5c37d2fafd\n",
            "24/07/28 20:48:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-19b860da-ca68-470d-bd20-5ac012268da2\n",
            "SPARQLGX version 1.1\n",
            "\n",
            "Usage:\n",
            "   bin/sparqlgx.sh light-load dbName tripleFile_HDFSPath\n",
            "   bin/sparqlgx.sh load dbName tripleFile_HDFSPath\n",
            "   bin/sparqlgx.sh generate-stat dbName tripleFile_HDFSPath\n",
            "   bin/sparqlgx.sh query [-o responseFile_HDFSPath] [--no-optim] [--stat] [--clean] dbName queryFile_LocalPath\n",
            "   bin/sparqlgx.sh direct-query [-o responseFile_HDFSPath] [--no-optim] [--clean] queryFile_LocalPath tripleFile_HDFSPath\n",
            "   bin/sparqlgx.sh translate [--no-optim] [--stat] dbName queryFile_LocalPath\n",
            "   bin/sparqlgx.sh remove dbName\n",
            "\n",
            "   bin/sparqlgx.sh --version\n",
            "   bin/sparqlgx.sh --help\n",
            "\n",
            "Tyrex <tyrex.inria.fr>\n",
            "2017\n",
            "24/07/28 20:48:26 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/07/28 20:48:26 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:48:26 INFO SparkContext: Java version 11.0.23\n",
            "24/07/28 20:48:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/07/28 20:48:26 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:48:26 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/07/28 20:48:26 INFO ResourceUtils: ==============================================================\n",
            "24/07/28 20:48:26 INFO SparkContext: Submitted application: Simple Application\n",
            "24/07/28 20:48:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/07/28 20:48:26 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/07/28 20:48:26 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/07/28 20:48:26 INFO SecurityManager: Changing view acls to: root\n",
            "24/07/28 20:48:26 INFO SecurityManager: Changing modify acls to: root\n",
            "24/07/28 20:48:26 INFO SecurityManager: Changing view acls groups to: \n",
            "24/07/28 20:48:26 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/07/28 20:48:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/07/28 20:48:27 INFO Utils: Successfully started service 'sparkDriver' on port 46161.\n",
            "24/07/28 20:48:27 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/07/28 20:48:27 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/07/28 20:48:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/07/28 20:48:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/07/28 20:48:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/07/28 20:48:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-149ce1bf-2095-452a-9a5f-08d74e8d0566\n",
            "24/07/28 20:48:27 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n",
            "24/07/28 20:48:27 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/07/28 20:48:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/07/28 20:48:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/07/28 20:48:28 INFO SparkContext: Added JAR file:/content/sparqlgx/bin/sparqlgx-load.jar at spark://4ae5c1b8e490:46161/jars/sparqlgx-load.jar with timestamp 1722199706318\n",
            "24/07/28 20:48:28 INFO Executor: Starting executor ID driver on host 4ae5c1b8e490\n",
            "24/07/28 20:48:28 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "24/07/28 20:48:28 INFO Executor: Java version 11.0.23\n",
            "24/07/28 20:48:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/07/28 20:48:28 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e3ecf5c for default.\n",
            "24/07/28 20:48:28 INFO Executor: Fetching spark://4ae5c1b8e490:46161/jars/sparqlgx-load.jar with timestamp 1722199706318\n",
            "24/07/28 20:48:28 INFO TransportClientFactory: Successfully created connection to 4ae5c1b8e490/172.28.0.12:46161 after 67 ms (0 ms spent in bootstraps)\n",
            "24/07/28 20:48:28 INFO Utils: Fetching spark://4ae5c1b8e490:46161/jars/sparqlgx-load.jar to /tmp/spark-ee324d33-f162-45b9-8ff6-ceae51344ec7/userFiles-ce839cde-8f6e-4895-8330-5649428fa2db/fetchFileTemp14339170557072331159.tmp\n",
            "24/07/28 20:48:28 INFO Executor: Adding file:/tmp/spark-ee324d33-f162-45b9-8ff6-ceae51344ec7/userFiles-ce839cde-8f6e-4895-8330-5649428fa2db/sparqlgx-load.jar to class loader default\n",
            "24/07/28 20:48:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34357.\n",
            "24/07/28 20:48:28 INFO NettyBlockTransferService: Server created on 4ae5c1b8e490:34357\n",
            "24/07/28 20:48:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/07/28 20:48:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ae5c1b8e490, 34357, None)\n",
            "24/07/28 20:48:28 INFO BlockManagerMasterEndpoint: Registering block manager 4ae5c1b8e490:34357 with 1048.8 MiB RAM, BlockManagerId(driver, 4ae5c1b8e490, 34357, None)\n",
            "24/07/28 20:48:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ae5c1b8e490, 34357, None)\n",
            "24/07/28 20:48:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ae5c1b8e490, 34357, None)\n",
            "24/07/28 20:48:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:48:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 1048.6 MiB)\n",
            "24/07/28 20:48:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4ae5c1b8e490:34357 (size: 32.6 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:30 INFO SparkContext: Created broadcast 0 from textFile at Load.scala:351\n",
            "24/07/28 20:48:30 INFO FileInputFormat: Total input files to process : 1\n",
            "24/07/28 20:48:30 INFO SparkContext: Starting job: reduce at Load.scala:223\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Registering RDD 5 (map at Load.scala:221) as input to shuffle 0\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Got job 0 (reduce at Load.scala:223) with 7 output partitions\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at Load.scala:223)\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221), which has no missing parents\n",
            "24/07/28 20:48:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.4 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4ae5c1b8e490:34357 (size: 4.2 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/07/28 20:48:31 INFO DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at Load.scala:221) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\n",
            "24/07/28 20:48:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 7 tasks resource profile 0\n",
            "24/07/28 20:48:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4ae5c1b8e490, executor driver, partition 0, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:32 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (4ae5c1b8e490, executor driver, partition 1, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:32 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/07/28 20:48:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/07/28 20:48:32 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:0+33554432\n",
            "24/07/28 20:48:32 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:33554432+33554432\n",
            "24/07/28 20:48:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:41 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (4ae5c1b8e490, executor driver, partition 2, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:41 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "24/07/28 20:48:41 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:67108864+33554432\n",
            "24/07/28 20:48:41 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9453 ms on 4ae5c1b8e490 (executor driver) (1/7)\n",
            "24/07/28 20:48:41 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (4ae5c1b8e490, executor driver, partition 3, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:41 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "24/07/28 20:48:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 9354 ms on 4ae5c1b8e490 (executor driver) (2/7)\n",
            "24/07/28 20:48:41 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:100663296+33554432\n",
            "24/07/28 20:48:44 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:44 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (4ae5c1b8e490, executor driver, partition 4, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:44 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
            "24/07/28 20:48:44 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3068 ms on 4ae5c1b8e490 (executor driver) (3/7)\n",
            "24/07/28 20:48:44 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:134217728+33554432\n",
            "24/07/28 20:48:45 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1308 bytes result sent to driver\n",
            "24/07/28 20:48:45 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (4ae5c1b8e490, executor driver, partition 5, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:45 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 3563 ms on 4ae5c1b8e490 (executor driver) (4/7)\n",
            "24/07/28 20:48:45 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
            "24/07/28 20:48:45 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:167772160+33554432\n",
            "24/07/28 20:48:47 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:47 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (4ae5c1b8e490, executor driver, partition 6, PROCESS_LOCAL, 7855 bytes) \n",
            "24/07/28 20:48:47 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 3057 ms on 4ae5c1b8e490 (executor driver) (5/7)\n",
            "24/07/28 20:48:47 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
            "24/07/28 20:48:47 INFO HadoopRDD: Input split: file:/rdf_data/enriched_ppio_ontology_with_kegg_Pathways.nt:201326592+28756888\n",
            "24/07/28 20:48:47 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:47 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2854 ms on 4ae5c1b8e490 (executor driver) (6/7)\n",
            "24/07/28 20:48:48 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1351 bytes result sent to driver\n",
            "24/07/28 20:48:48 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 1357 ms on 4ae5c1b8e490 (executor driver) (7/7)\n",
            "24/07/28 20:48:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:48:48 INFO DAGScheduler: ShuffleMapStage 0 (map at Load.scala:221) finished in 17.388 s\n",
            "24/07/28 20:48:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/07/28 20:48:48 INFO DAGScheduler: running: Set()\n",
            "24/07/28 20:48:48 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/07/28 20:48:48 INFO DAGScheduler: failed: Set()\n",
            "24/07/28 20:48:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at map at Load.scala:223), which has no missing parents\n",
            "24/07/28 20:48:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.8 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 1048.5 MiB)\n",
            "24/07/28 20:48:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4ae5c1b8e490:34357 (size: 3.3 KiB, free: 1048.8 MiB)\n",
            "24/07/28 20:48:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/07/28 20:48:49 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at map at Load.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\n",
            "24/07/28 20:48:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0\n",
            "24/07/28 20:48:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 7) (4ae5c1b8e490, executor driver, partition 0, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 8) (4ae5c1b8e490, executor driver, partition 1, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:49 INFO Executor: Running task 1.0 in stage 1.0 (TID 8)\n",
            "24/07/28 20:48:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 7)\n",
            "24/07/28 20:48:49 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:49 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms\n",
            "24/07/28 20:48:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n",
            "24/07/28 20:48:49 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 917.8 KiB, free 1047.6 MiB)\n",
            "24/07/28 20:48:49 INFO BlockManagerInfo: Added rdd_6_0 in memory on 4ae5c1b8e490:34357 (size: 917.8 KiB, free: 1047.9 MiB)\n",
            "24/07/28 20:48:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 7). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 9) (4ae5c1b8e490, executor driver, partition 2, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 7) in 906 ms on 4ae5c1b8e490 (executor driver) (1/7)\n",
            "24/07/28 20:48:49 INFO Executor: Running task 2.0 in stage 1.0 (TID 9)\n",
            "24/07/28 20:48:49 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 911.7 KiB, free 1046.7 MiB)\n",
            "24/07/28 20:48:49 INFO BlockManagerInfo: Added rdd_6_1 in memory on 4ae5c1b8e490:34357 (size: 911.7 KiB, free: 1047.0 MiB)\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "24/07/28 20:48:50 INFO Executor: Finished task 1.0 in stage 1.0 (TID 8). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:50 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 10) (4ae5c1b8e490, executor driver, partition 3, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:50 INFO Executor: Running task 3.0 in stage 1.0 (TID 10)\n",
            "24/07/28 20:48:50 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 8) in 975 ms on 4ae5c1b8e490 (executor driver) (2/7)\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "24/07/28 20:48:50 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 927.5 KiB, free 1045.8 MiB)\n",
            "24/07/28 20:48:50 INFO BlockManagerInfo: Added rdd_6_2 in memory on 4ae5c1b8e490:34357 (size: 927.5 KiB, free: 1046.1 MiB)\n",
            "24/07/28 20:48:50 INFO MemoryStore: Block rdd_6_3 stored as values in memory (estimated size 920.8 KiB, free 1044.9 MiB)\n",
            "24/07/28 20:48:50 INFO BlockManagerInfo: Added rdd_6_3 in memory on 4ae5c1b8e490:34357 (size: 920.8 KiB, free: 1045.2 MiB)\n",
            "24/07/28 20:48:50 INFO Executor: Finished task 2.0 in stage 1.0 (TID 9). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:50 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 11) (4ae5c1b8e490, executor driver, partition 4, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:50 INFO Executor: Running task 4.0 in stage 1.0 (TID 11)\n",
            "24/07/28 20:48:50 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 9) in 806 ms on 4ae5c1b8e490 (executor driver) (3/7)\n",
            "24/07/28 20:48:50 INFO Executor: Finished task 3.0 in stage 1.0 (TID 10). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:50 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 12) (4ae5c1b8e490, executor driver, partition 5, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:50 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 10) in 750 ms on 4ae5c1b8e490 (executor driver) (4/7)\n",
            "24/07/28 20:48:50 INFO Executor: Running task 5.0 in stage 1.0 (TID 12)\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Getting 7 (124.8 KiB) non-empty blocks including 7 (124.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Getting 7 (123.0 KiB) non-empty blocks including 7 (123.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
            "24/07/28 20:48:51 INFO MemoryStore: Block rdd_6_4 stored as values in memory (estimated size 917.6 KiB, free 1044.0 MiB)\n",
            "24/07/28 20:48:51 INFO BlockManagerInfo: Added rdd_6_4 in memory on 4ae5c1b8e490:34357 (size: 917.6 KiB, free: 1044.3 MiB)\n",
            "24/07/28 20:48:51 INFO MemoryStore: Block rdd_6_5 stored as values in memory (estimated size 921.4 KiB, free 1043.1 MiB)\n",
            "24/07/28 20:48:51 INFO BlockManagerInfo: Added rdd_6_5 in memory on 4ae5c1b8e490:34357 (size: 921.4 KiB, free: 1043.4 MiB)\n",
            "24/07/28 20:48:51 INFO Executor: Finished task 4.0 in stage 1.0 (TID 11). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:51 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 13) (4ae5c1b8e490, executor driver, partition 6, NODE_LOCAL, 7609 bytes) \n",
            "24/07/28 20:48:51 INFO Executor: Running task 6.0 in stage 1.0 (TID 13)\n",
            "24/07/28 20:48:51 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 11) in 608 ms on 4ae5c1b8e490 (executor driver) (5/7)\n",
            "24/07/28 20:48:51 INFO Executor: Finished task 5.0 in stage 1.0 (TID 12). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:51 INFO ShuffleBlockFetcherIterator: Getting 7 (124.8 KiB) non-empty blocks including 7 (124.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/07/28 20:48:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "24/07/28 20:48:51 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 12) in 615 ms on 4ae5c1b8e490 (executor driver) (6/7)\n",
            "24/07/28 20:48:51 INFO MemoryStore: Block rdd_6_6 stored as values in memory (estimated size 913.5 KiB, free 1042.3 MiB)\n",
            "24/07/28 20:48:51 INFO BlockManagerInfo: Added rdd_6_6 in memory on 4ae5c1b8e490:34357 (size: 913.5 KiB, free: 1042.5 MiB)\n",
            "24/07/28 20:48:51 INFO Executor: Finished task 6.0 in stage 1.0 (TID 13). 1873 bytes result sent to driver\n",
            "24/07/28 20:48:51 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 13) in 361 ms on 4ae5c1b8e490 (executor driver) (7/7)\n",
            "24/07/28 20:48:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/07/28 20:48:51 INFO DAGScheduler: ResultStage 1 (reduce at Load.scala:223) finished in 2.703 s\n",
            "24/07/28 20:48:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/07/28 20:48:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/07/28 20:48:51 INFO DAGScheduler: Job 0 finished: reduce at Load.scala:223, took 21.004572 s\n",
            "Exception in thread \"main\" java.lang.NoSuchMethodError: 'scala.collection.mutable.ArrayOps scala.Predef$.refArrayOps(java.lang.Object[])'\n",
            "\tat Main$.prefix(Load.scala:231)\n",
            "\tat Main$.main(Load.scala:365)\n",
            "\tat Main.main(Load.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "24/07/28 20:48:51 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/07/28 20:48:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/07/28 20:48:51 INFO SparkUI: Stopped Spark web UI at http://4ae5c1b8e490:4040\n",
            "24/07/28 20:48:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/07/28 20:48:51 INFO MemoryStore: MemoryStore cleared\n",
            "24/07/28 20:48:51 INFO BlockManager: BlockManager stopped\n",
            "24/07/28 20:48:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/07/28 20:48:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/07/28 20:48:52 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/07/28 20:48:52 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/07/28 20:48:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-caf92bbd-f4ea-4d9c-8991-90711de31315\n",
            "24/07/28 20:48:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee324d33-f162-45b9-8ff6-ceae51344ec7\n",
            "bin/sgx-eval.sh: line 94: bin/sparqlgx-translator: No such file or directory\n",
            "[sbt_options] declare -a sbt_options=()\n",
            "[process_args] java_version = '11'\n",
            "[copyRt] java9_rt = '/root/.sbt/1.0/java9-rt-ext-ubuntu_11_0_23/rt.jar'\n",
            "# Executing command line:\n",
            "java\n",
            "-Dfile.encoding=UTF-8\n",
            "-Xms1024m\n",
            "-Xmx1024m\n",
            "-Xss4M\n",
            "-XX:ReservedCodeCacheSize=128m\n",
            "-Dsbt.script=/usr/bin/sbt\n",
            "-Dscala.ext.dirs=/root/.sbt/1.0/java9-rt-ext-ubuntu_11_0_23\n",
            "-jar\n",
            "/root/.cache/sbt/boot/sbt-launch/1.10.1/sbt-launch-1.10.1.jar\n",
            "clean\n",
            "package\n",
            "\n",
            "[info] welcome to sbt 1.10.1 (Ubuntu Java 11.0.23)\n",
            "[info] loading project definition from /root/.sparqlgx/eval/project\n",
            "[info] loading settings for project eval from build.sbt ...\n",
            "[info] set current project to SPARQLGX Evaluation (in build file:/root/.sparqlgx/eval/)\n",
            "[success] Total time: 1 s, completed Jul 28, 2024, 8:49:07 PM\n",
            "[info] compiling 1 Scala source to /root/.sparqlgx/eval/target/scala-2.11/classes ...\n",
            "[info] done compiling\n",
            "[success] Total time: 15 s, completed Jul 28, 2024, 8:49:22 PM\n",
            "Error: Failed to load class Query.\n",
            "24/07/28 20:49:28 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/07/28 20:49:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-d4eb63b5-6fac-4e21-9b39-fb4cc845e7cf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sparqlgx/query_output.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-09194621111d>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 打印查询结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparqlgx/query_output.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sparqlgx/query_output.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EblCjU-kzc8c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqc2o8e_zc-z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s74Jbst7zdBT"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}